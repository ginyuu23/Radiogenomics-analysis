{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "import warnings; warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOPX-2 image features\n",
    "def expression_type(s):\n",
    "    it = {b'normal/over':0, b'low':1}\n",
    "    return it[s]\n",
    "\n",
    "pathtrain = \"/path to the train data/\"\n",
    "pathtest = \"/path to the test data/\"\n",
    "datatrain = np.loadtxt(pathtrain, dtype=float, delimiter='\\t', converters={2:expression_type})\n",
    "datatest = np.loadtxt(pathtest, dtype=float, delimiter='\\t', converters={2:expression_type})\n",
    "\n",
    "train_data_nosmo, train_label_nosmo=np.split(datatrain,indices_or_sections=(2,),axis=1) #x为数据，y为标签,axis是分割的方向，1表示横向，0表示纵向，默认为0\n",
    "#train_data, train_label=np.split(datatrain,indices_or_sections=(8,),axis=1) #x为数据，y为标签,axis是分割的方向，1表示横向，0表示纵向，默认为0\n",
    "test_data, test_label=np.split(datatest,indices_or_sections=(2,),axis=1) #x为数据，y为标签,axis是分割的方向，1表示横向，0表示纵向，默认为0\n",
    "#train_data:训练样本，test_data：测试样本，train_label：训练样本标签，test_label：测试样本标签\n",
    "\n",
    "smo = SMOTE(random_state=1,k_neighbors=5)\n",
    "#smo = BorderlineSMOTE(kind='borderline-1',random_state=38, k_neighbors=3)\n",
    "train_data, train_label = smo.fit_resample(train_data_nosmo, train_label_nosmo)\n",
    "\n",
    "#svmsmo = SVMSMOTE(random_state=4, k_neighbors=5)\n",
    "#train_data, train_label = svmsmo.fit_resample(train_data_nosmo, train_label_nosmo)\n",
    "\n",
    "#ada = ADASYN(random_state=8,n_neighbors=3)\n",
    "#train_data, train_label = ada.fit_resample(train_data_nosmo, train_label_nosmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=100\n",
    "train_data_1, valid_data_1, train_label_1, valid_label_1 = train_test_split(train_data_all, train_label_all, test_size=0.2, random_state=i)\n",
    "train_data_2, valid_data_2, train_label_2, valid_label_2 = train_test_split(train_data_1, train_label_1, test_size=0.25, random_state=i)\n",
    "train_data_3, valid_data_3, train_label_3, valid_label_3 = train_test_split(train_data_2, train_label_2, test_size=1/3, random_state=i)\n",
    "train_data_4, valid_data_4, train_label_4, valid_label_4 = train_test_split(train_data_3, train_label_3, test_size=0.5, random_state=i)\n",
    "\n",
    "print(valid_data_1.shape, valid_data_2.shape, valid_data_3.shape, valid_data_4.shape, train_data_4.shape, train_data_all.shape)\n",
    "print(valid_label_1.shape, valid_label_2.shape, valid_label_3.shape, valid_label_4.shape, train_label_4.shape, train_label_all.shape)\n",
    "\n",
    "\n",
    "#print(train_data_nosmo.shape, train_data_all.shape, train_data.shape, valid_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第1段階のモデル作成\n",
    "#1\n",
    "first_model_1 = svm.SVC(C=5.5031,kernel='rbf',gamma=0.8459,decision_function_shape='ovo',probability=True) # accuracy\n",
    "#first_model_1 = svm.SVC(C=299.1,kernel='rbf',gamma=0.000052,decision_function_shape='ovo',probability=True)\n",
    "#first_model_2 = RandomForestClassifier(random_state=556,max_depth=15,max_features=0.544982,min_samples_split=2, n_estimators=31)\n",
    "first_model_2 = RandomForestClassifier(random_state=393,max_depth=10,max_features=0.5,min_samples_split=11, n_estimators=236)\n",
    "first_model_3 = GradientBoostingClassifier(random_state=691,learning_rate=0.1,max_depth=3, max_features=0.9,min_samples_split=6,n_estimators=184,subsample=0.9)\n",
    "first_model_1.fit(train_data_1, train_label_1)\n",
    "first_model_2.fit(train_data_1, train_label_1)\n",
    "first_model_3.fit(train_data_1, train_label_1)\n",
    "\n",
    "#2\n",
    "first_model_1_2 = svm.SVC(C=5.5031,kernel='rbf',gamma=0.8459,decision_function_shape='ovo',probability=True)\n",
    "first_model_2_2 = RandomForestClassifier(random_state=393,max_depth=10,max_features=0.5,min_samples_split=11, n_estimators=236)\n",
    "first_model_3_2 = GradientBoostingClassifier(random_state=691,learning_rate=0.1,max_depth=3, max_features=0.9,min_samples_split=6,n_estimators=184,subsample=0.9)\n",
    "train2 = np.row_stack((valid_data_1, train_data_2))\n",
    "label2 = np.concatenate([valid_label_1, train_label_2],axis=0)\n",
    "first_model_1_2.fit(train2, label2)\n",
    "first_model_2_2.fit(train2, label2)\n",
    "first_model_3_2.fit(train2, label2)\n",
    "\n",
    "#3\n",
    "first_model_1_3 = svm.SVC(C=5.5031,kernel='rbf',gamma=0.8459,decision_function_shape='ovo',probability=True)\n",
    "first_model_2_3 = RandomForestClassifier(random_state=393,max_depth=10,max_features=0.5,min_samples_split=11, n_estimators=236)\n",
    "first_model_3_3 = GradientBoostingClassifier(random_state=691,learning_rate=0.1,max_depth=3, max_features=0.9,min_samples_split=6,n_estimators=184,subsample=0.9)\n",
    "train3 = np.row_stack((valid_data_2, valid_data_1,train_data_3))\n",
    "label3 = np.concatenate([valid_label_2, valid_label_1,train_label_3],axis=0)\n",
    "first_model_1_3.fit(train3, label3)\n",
    "first_model_2_3.fit(train3, label3)\n",
    "first_model_3_3.fit(train3, label3)\n",
    "\n",
    "#4\n",
    "first_model_1_4 = svm.SVC(C=5.5031,kernel='rbf',gamma=0.8459,decision_function_shape='ovo',probability=True)\n",
    "first_model_2_4 = RandomForestClassifier(random_state=393,max_depth=10,max_features=0.5,min_samples_split=11, n_estimators=236)\n",
    "first_model_3_4 = GradientBoostingClassifier(random_state=691,learning_rate=0.1,max_depth=3, max_features=0.9,min_samples_split=6,n_estimators=184,subsample=0.9)\n",
    "train4 = np.row_stack((valid_data_3,valid_data_2, valid_data_1,train_data_4))\n",
    "label4 = np.concatenate([valid_label_3,valid_label_2, valid_label_1,train_label_4],axis=0)\n",
    "first_model_1_4.fit(train4, label4)\n",
    "first_model_2_4.fit(train4, label4)\n",
    "first_model_3_4.fit(train4, label4)\n",
    "#5\n",
    "first_model_1_5 = svm.SVC(C=5.5031,kernel='rbf',gamma=0.8459,decision_function_shape='ovo',probability=True)\n",
    "first_model_2_5 = RandomForestClassifier(random_state=393,max_depth=10,max_features=0.5,min_samples_split=11, n_estimators=236)\n",
    "first_model_3_5 = GradientBoostingClassifier(random_state=691,learning_rate=0.1,max_depth=3, max_features=0.9,min_samples_split=6,n_estimators=184,subsample=0.9)\n",
    "train5 = np.row_stack((valid_data_4,valid_data_3,valid_data_2, valid_data_1))\n",
    "label5 = np.concatenate([valid_label_4,valid_label_3,valid_label_2, valid_label_1],axis=0)\n",
    "first_model_1_5.fit(train5, label5)\n",
    "first_model_2_5.fit(train5, label5)\n",
    "first_model_3_5.fit(train5, label5)\n",
    "# 結果の検証 \n",
    "test_pred_1 = first_model_1.predict(test_data)\n",
    "test_pred_2 = first_model_2.predict(test_data)\n",
    "test_pred_3 = first_model_3.predict(test_data)\n",
    "\n",
    "\n",
    "#　各モデル個別の予測精度を平均二乗誤差で確認\n",
    "print (\"モデル1の平均2乗誤差: {:.4f}\".format(mean_squared_error(test_label, test_pred_1)))\n",
    "print (\"モデル2の平均2乗誤差: {:.4f}\".format(mean_squared_error(test_label, test_pred_2)))\n",
    "print (\"モデル2の平均2乗誤差: {:.4f}\".format(mean_squared_error(test_label, test_pred_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スタッキングによる予測 Prediction using a stacking ensemble learning \n",
    "\n",
    "# 第1段階の予測値(この後、メタモデルの入力に使用) Predictions for validation data\n",
    "first_pred_1 = first_model_1.predict(valid_data_1)\n",
    "first_pred_2 = first_model_2.predict(valid_data_1)\n",
    "first_pred_3 = first_model_2.predict(valid_data_1)\n",
    "first_pred_1_2 = first_model_1_2.predict(valid_data_2)\n",
    "first_pred_2_2 = first_model_2_2.predict(valid_data_2)\n",
    "first_pred_3_2 = first_model_2_2.predict(valid_data_2)\n",
    "first_pred_1_3 = first_model_1_3.predict(valid_data_3)\n",
    "first_pred_2_3 = first_model_2_3.predict(valid_data_3)\n",
    "first_pred_3_3 = first_model_2_3.predict(valid_data_3)\n",
    "first_pred_1_4 = first_model_1_4.predict(valid_data_4)\n",
    "first_pred_2_4 = first_model_2_4.predict(valid_data_4)\n",
    "first_pred_3_4 = first_model_2_4.predict(valid_data_4)\n",
    "first_pred_1_5 = first_model_1_5.predict(train_data_4)\n",
    "first_pred_2_5 = first_model_2_5.predict(train_data_4)\n",
    "first_pred_3_5 = first_model_2_5.predict(train_data_4)\n",
    "\n",
    "#第1段階の予測値を積み重ねる Stacking predictions by three base models for validation data\n",
    "stack_pred_1 = np.column_stack((first_pred_1, first_pred_2,first_pred_3))\n",
    "stack_pred_2 = np.column_stack((first_pred_1_2, first_pred_2_2,first_pred_3_2))\n",
    "stack_pred_3 = np.column_stack((first_pred_1_3, first_pred_2_3,first_pred_3_3))\n",
    "stack_pred_4 = np.column_stack((first_pred_1_4, first_pred_2_4,first_pred_3_4))\n",
    "stack_pred_5 = np.column_stack((first_pred_1_5, first_pred_2_5,first_pred_3_5))\n",
    "stack_pred_fin = np.row_stack((stack_pred_1,stack_pred_2,stack_pred_3,stack_pred_4,stack_pred_5))\n",
    "\n",
    "stack_pred_label_fin = np.concatenate([valid_label_1,valid_label_2,valid_label_3,valid_label_4,train_label_4],axis=0)\n",
    "\n",
    "print(stack_pred_fin.shape)\n",
    "print(stack_pred_label_fin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メタモデルの学習 Training a meta model using validation data\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(stack_pred_fin, stack_pred_label_fin)\n",
    "#meta_model.fit(stack_pred_1, valid_label_1)\n",
    "\n",
    "# 各モデルの検証データを積み重ねる Stacking predictions by three base models for test data\n",
    "stack_test_pred = np.column_stack((test_pred_1,test_pred_2,test_pred_3))\n",
    "\n",
    "#print(\"stack_test_pred.shape\", stack_test_pred.shape)\n",
    "#print(stack_test_pred) # matrix size: (Number of test data) X (Number of base models)\n",
    "\n",
    "# スタッキングの検証 Test of a meta model\n",
    "meta_test_pred = meta_model.predict(stack_test_pred)\n",
    "#print (\"メタモデルの平均2乗誤差: {:.4f}\".format(mean_squared_error(test_label, meta_test_pred)))\n",
    "\n",
    "\n",
    "cm1 = confusion_matrix(test_label,meta_test_pred)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "total1=sum(sum(cm1))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Specificity : ', specificity1 )\n",
    "sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Sensitivity : ', sensitivity1)\n",
    "    \n",
    "prob_predict_y_validation = meta_model.predict_proba(stack_test_pred)#给出带有概率值的结果，每个点所有label的概率和为1\n",
    "accuracy = meta_model.score(stack_test_pred,test_label)\n",
    "predictions_validation = prob_predict_y_validation[:, 1]\n",
    "fpr, tpr, _ = roc_curve(test_label, predictions_validation)###计算真正率和假正率\n",
    "roc_auc = auc(fpr, tpr)##计算auc的值\n",
    "#print(\"-----------------------------------------------------------------------\")\n",
    "#print(\"Accuracy=\",accuracy)\n",
    "print(\"testAUC=\",auc(fpr, tpr))\n",
    "print(meta_model.predict(stack_test_pred))\n",
    "\n",
    "\n",
    "meta_train_pred = meta_model.predict(stack_pred_fin)\n",
    "cm1 = confusion_matrix(stack_pred_label_fin,meta_train_pred)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "total1=sum(sum(cm1))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "specificity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Specificity : ', specificity1 )\n",
    "sensitivity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Sensitivity : ', sensitivity1)\n",
    "\n",
    "prob_predict_y_validation2 = meta_model.predict_proba(stack_pred_fin)#给出带有概率值的结果，每个点所有label的概率和为1\n",
    "accuracy_train = meta_model.score(stack_pred_fin,stack_pred_label_fin)\n",
    "predictions_validation_train = prob_predict_y_validation2[:, 1]\n",
    "fpr2, tpr2, _ = roc_curve(stack_pred_label_fin, predictions_validation_train)###计算真正率和假正率\n",
    "roc_auc2 = auc(fpr2, tpr2)##计算auc的值\n",
    "#print(\"-----------------------------------------------------------------------\")\n",
    "#print(\"Accuracy_train=\",accuracy_train)\n",
    "print(\"trainAUC=\",auc(fpr2, tpr2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
